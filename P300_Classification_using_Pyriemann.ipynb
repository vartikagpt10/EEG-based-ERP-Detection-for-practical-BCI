{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "P300 Classification using Pyriemann",
      "provenance": [],
      "collapsed_sections": [
        "zbT268a2eqzL"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vartikagpt10/EEG-based-ERP-Detection-for-practical-BCI/blob/main/P300_Classification_using_Pyriemann.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbT268a2eqzL"
      },
      "source": [
        "# Download and Extract Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Pt_de-II0NC",
        "outputId": "45b24a30-1f44-486c-9a09-f00028358607"
      },
      "source": [
        "!pip install --upgrade scikit-learn\n",
        "!pip install --upgrade gdown pyriemann tensorflow imbalanced-learn xgboost\n",
        "!gdown https://drive.google.com/uc?id=16x3lxR6KBc4OQv83voXnE88J0KzbeLFx -O /tmp/Data.zip\n",
        "\n",
        "import zipfile\n",
        "local_zip = '//tmp/Data.zip'\n",
        "\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "\n",
        "zip_ref.extractall('/tmp/Data')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.23.2)\n",
            "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.17.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already up-to-date: gdown in /usr/local/lib/python3.6/dist-packages (3.12.2)\n",
            "Requirement already up-to-date: pyriemann in /usr/local/lib/python3.6/dist-packages (0.2.6)\n",
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already up-to-date: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already up-to-date: xgboost in /usr/local/lib/python3.6/dist-packages (1.2.1)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from gdown) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: requests[socks] in /usr/local/lib/python3.6/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from pyriemann) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pyriemann) (0.23.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from pyriemann) (0.17.0)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from pyriemann) (1.1.4)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from pyriemann) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.35.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.33.2)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pyriemann) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->pyriemann) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->pyriemann) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (50.3.2)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16x3lxR6KBc4OQv83voXnE88J0KzbeLFx\n",
            "To: /tmp/Data.zip\n",
            "301MB [00:01, 292MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOOi1mRFOBs9"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import os\n",
        "from scipy.signal import filtfilt, cheby2\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkTA6-eNNqgh"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QN3hXKNoh5Y"
      },
      "source": [
        "train_folder_path = '/tmp/Data/Training set/'\n",
        "validation_folder_path = '/tmp/Data/Validation set/'\n",
        "test_folder_path = '/tmp/Data/Test set/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir_rf2xIemy2"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzUZV2RWQmcY",
        "outputId": "b2272a7d-a7b5-4d0f-c962-64d9cc3cf262"
      },
      "source": [
        "c=0\n",
        "x=os.listdir(train_folder_path)\n",
        "x.sort()\n",
        "print(x)\n",
        "for i in x:\n",
        "  c+=1\n",
        "  if c==1:\n",
        "    temp=sio.loadmat(train_folder_path+i)\n",
        "    dat=np.squeeze(temp['epo_tr']['x']).tolist()\n",
        "    dat=np.array(dat)\n",
        "    lab=np.squeeze(temp['epo_tr']['y']).tolist()\n",
        "    lab=np.array(lab)[0,:]\n",
        "    training_data=dat\n",
        "    training_labels=lab\n",
        "    continue\n",
        "  temp=sio.loadmat(train_folder_path+i)\n",
        "  dat=np.squeeze(temp['epo_tr']['x']).tolist()\n",
        "  dat=np.array(dat)\n",
        "  lab=np.squeeze(temp['epo_tr']['y']).tolist()\n",
        "  lab=np.array(lab)[0,:]\n",
        "  training_data=np.concatenate((training_data,dat),axis=2)\n",
        "  training_labels=np.concatenate((training_labels,lab))\n",
        "training_data=training_data.reshape((2700,56,100))\n",
        "training_data=training_data[:,:32,:]\n",
        "print(\"Training data shape:\",training_data.shape)\n",
        "print(\"Training labels shape:\",training_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Data_Sample01.mat', 'Data_Sample02.mat', 'Data_Sample03.mat', 'Data_Sample04.mat', 'Data_Sample05.mat', 'Data_Sample06.mat', 'Data_Sample07.mat', 'Data_Sample08.mat', 'Data_Sample09.mat', 'Data_Sample10.mat', 'Data_Sample11.mat', 'Data_Sample12.mat', 'Data_Sample13.mat', 'Data_Sample14.mat', 'Data_Sample15.mat']\n",
            "Training data shape: (2700, 32, 100)\n",
            "Training labels shape: (2700,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KILTNPQNm8z",
        "outputId": "64c0eb34-319a-4dee-cee6-9ea8c2173546"
      },
      "source": [
        "c=0\n",
        "x=os.listdir(validation_folder_path)\n",
        "x.sort()\n",
        "print(x)\n",
        "for i in x:\n",
        "  c+=1\n",
        "  if c==1:\n",
        "    temp=sio.loadmat(validation_folder_path+i)\n",
        "    dat=np.squeeze(temp['epo_val']['x']).tolist()\n",
        "    dat=np.array(dat)\n",
        "    lab=np.squeeze(temp['epo_val']['y']).tolist()\n",
        "    lab=np.array(lab)[0,:]\n",
        "    validation_data=dat\n",
        "    validation_labels=lab\n",
        "    continue\n",
        "  temp=sio.loadmat(validation_folder_path+i)\n",
        "  dat=np.squeeze(temp['epo_val']['x']).tolist()\n",
        "  dat=np.array(dat)\n",
        "  lab=np.squeeze(temp['epo_val']['y']).tolist()\n",
        "  lab=np.array(lab)[0,:]\n",
        "  validation_data=np.concatenate((validation_data,dat),axis=2)\n",
        "  validation_labels=np.concatenate((validation_labels,lab))\n",
        "validation_data=validation_data.reshape((900,56,100))\n",
        "validation_data=validation_data[:,:32,:]\n",
        "print(\"Validation data shape:\",validation_data.shape)\n",
        "print(\"Validation labels shape:\",validation_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Data_Sample01.mat', 'Data_Sample02.mat', 'Data_Sample03.mat', 'Data_Sample04.mat', 'Data_Sample05.mat', 'Data_Sample06.mat', 'Data_Sample07.mat', 'Data_Sample08.mat', 'Data_Sample09.mat', 'Data_Sample10.mat', 'Data_Sample11.mat', 'Data_Sample12.mat', 'Data_Sample13.mat', 'Data_Sample14.mat', 'Data_Sample15.mat']\n",
            "Validation data shape: (900, 32, 100)\n",
            "Validation labels shape: (900,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUoMZQ-1nPsI",
        "outputId": "585582bb-d822-4d61-b726-82908ff064af"
      },
      "source": [
        "c=0\n",
        "x=os.listdir(test_folder_path)\n",
        "x.sort()\n",
        "print(x)\n",
        "for i in x:\n",
        "  c+=1\n",
        "  if c==1:\n",
        "    temp=sio.loadmat(test_folder_path+i)\n",
        "    dat=np.squeeze(temp['epo_te']['x']).tolist()\n",
        "    dat=np.array(dat)\n",
        "\n",
        "    test_data=dat\n",
        "    test_labels=lab\n",
        "    continue\n",
        "  temp=sio.loadmat(test_folder_path+i)\n",
        "  dat=np.squeeze(temp['epo_te']['x']).tolist()\n",
        "  dat=np.array(dat)\n",
        "\n",
        "  test_data=np.concatenate((test_data,dat),axis=2)\n",
        "\n",
        "test_data=test_data.reshape((900,56,100))\n",
        "test_data=test_data[:,:32,:]\n",
        "print(\"Test data shape:\",test_data.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Data_Sample01.mat', 'Data_Sample02.mat', 'Data_Sample03.mat', 'Data_Sample04.mat', 'Data_Sample05.mat', 'Data_Sample06.mat', 'Data_Sample07.mat', 'Data_Sample08.mat', 'Data_Sample09.mat', 'Data_Sample10.mat', 'Data_Sample11.mat', 'Data_Sample12.mat', 'Data_Sample13.mat', 'Data_Sample14.mat', 'Data_Sample15.mat']\n",
            "Test data shape: (900, 32, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIPSLhEeLV0X"
      },
      "source": [
        "# dat=np.squeeze(temp['epo_val']['y']).tolist()\n",
        "# dat=np.array(dat)[0,:]\n",
        "# dat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEPY4yx-eh95"
      },
      "source": [
        "# Filtering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWaU1-3gWQxh"
      },
      "source": [
        "def chebyBandpass(signal, lowcut, fs, order):\n",
        "  nyq=0.5*fs\n",
        "  low = lowcut/nyq\n",
        "  # high = highcut / nyq\n",
        "  b, a = cheby2(order,12, low, btype='low')\n",
        "  filtered = filtfilt(b, a, signal)\n",
        "  return filtered\n",
        "\n",
        "def filterall(data):\n",
        "  filtered=[]\n",
        "  for trial in data:\n",
        "    temp_filtered=[]\n",
        "    for channel in trial:\n",
        "      temp_filtered.append(chebyBandpass(channel,20,100,3))\n",
        "    filtered.append(temp_filtered)\n",
        "  return np.array(filtered)\n",
        "\n",
        "training_data=filterall(training_data)\n",
        "validation_data=filterall(validation_data)\n",
        "test_data=filterall(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT-N01fOccCS"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Activation, Permute, Dropout\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import SpatialDropout2D\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.layers import Input, Flatten\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "\n",
        "from pyriemann.estimation import ERPCovariances\n",
        "from pyriemann.tangentspace import TangentSpace\n",
        "from pyriemann.classification import TSclassifier\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from imblearn.combine import SMOTETomek"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3ipNZR9_mbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77530df3-9842-4bcf-fd3a-0a37e4af616a"
      },
      "source": [
        "training_labels_all=training_labels\n",
        "validation_labels_all=validation_labels\n",
        "auc1=[]\n",
        "auc2=[]\n",
        "auc3=[]\n",
        "proba1=[]\n",
        "proba2=[]\n",
        "proba3=[]\n",
        "val_proba1=[]\n",
        "val_proba2=[]\n",
        "val_proba3=[]\n",
        "score1=[]\n",
        "score2=[]\n",
        "score3=[]\n",
        "for sub in range(15):\n",
        "  filt_train=training_data[(sub)*180:(sub+1)*180,:,:]\n",
        "  filt_val=validation_data[(sub)*60:(sub+1)*60,:,:]\n",
        "  filt_test=test_data[(sub)*60:(sub+1)*60,:,:]\n",
        "  training_labels=training_labels_all[(sub)*180:(sub+1)*180]\n",
        "  validation_labels=validation_labels_all[(sub)*60:(sub+1)*60]\n",
        "\n",
        "  print(filt_train.shape)\n",
        "  print(filt_val.shape)\n",
        "  print(filt_test.shape)\n",
        "\n",
        "  covmat = ERPCovariances(estimator='lwf').fit(filt_train, training_labels)\n",
        "  cov_train = covmat.transform(filt_train)\n",
        "  ts_train = TangentSpace().fit_transform(cov_train)\n",
        "\n",
        "  cov_val = covmat.transform(filt_val)\n",
        "  ts_val = TangentSpace().fit_transform(cov_val)\n",
        "\n",
        "  cov_test = covmat.transform(filt_test)\n",
        "  ts_test = TangentSpace().fit_transform(cov_test)\n",
        "\n",
        "  print(ts_train.shape)\n",
        "  print(training_labels.shape)\n",
        "  print(ts_val.shape)\n",
        "  print(validation_labels.shape)\n",
        "  print(ts_test.shape)\n",
        "  print(cov_train.shape)\n",
        "  print(cov_val.shape)\n",
        "  print(cov_test.shape)\n",
        "\n",
        "  training_labels_orig=training_labels\n",
        "  oversample=SMOTETomek()\n",
        "  ts_train, training_labels = oversample.fit_resample(ts_train,training_labels)\n",
        "######## MODEL 1 SVM ########\n",
        "  model = svm.SVC(probability=True)\n",
        "  model.fit(ts_train, training_labels)\n",
        "\n",
        "  training_predicted= model.predict(ts_train)\n",
        "  validation_pred=model.predict(ts_val)\n",
        "  score= validation_pred==validation_labels\n",
        "  score1.append(score.mean())\n",
        "  probs = model.predict_proba(ts_val)\n",
        "  val_proba1.append(probs)\n",
        "  preds = probs[:,1]\n",
        "  fpr, tpr, threshold = metrics.roc_curve(validation_labels, preds)\n",
        "  roc_auc = metrics.auc(fpr, tpr)\n",
        "  auc1.append(roc_auc)\n",
        "  proba1.append(model.predict_proba(ts_test))\n",
        "######## MODEL 2 xgboost ########\n",
        "  model = XGBClassifier(probability=True)\n",
        "  model.fit(ts_train, training_labels)\n",
        "\n",
        "  training_predicted= model.predict(ts_train)\n",
        "  validation_pred=model.predict(ts_val)\n",
        "  score= validation_pred==validation_labels\n",
        "  score2.append(score.mean())\n",
        "  probs = model.predict_proba(ts_val)\n",
        "  val_proba2.append(probs)\n",
        "  preds = probs[:,1]\n",
        "  fpr, tpr, threshold = metrics.roc_curve(validation_labels, preds)\n",
        "  roc_auc = metrics.auc(fpr, tpr)\n",
        "  auc2.append(roc_auc)\n",
        "  proba2.append(model.predict_proba(ts_test))\n",
        "######## MODEL 3 tsclassifier ########\n",
        "  training_labels=training_labels_orig\n",
        "  model = TSclassifier(clf=XGBClassifier())\n",
        "  model.fit(cov_train, training_labels)\n",
        "\n",
        "  training_predicted= model.predict(cov_train)\n",
        "  validation_pred=model.predict(cov_val)\n",
        "  score= validation_pred==validation_labels\n",
        "  score3.append(score.mean())\n",
        "  probs = model.predict_proba(cov_val)\n",
        "  val_proba3.append(probs)\n",
        "  preds = probs[:,1]\n",
        "  fpr, tpr, threshold = metrics.roc_curve(validation_labels, preds)\n",
        "  roc_auc = metrics.auc(fpr, tpr)\n",
        "  auc3.append(roc_auc)\n",
        "  proba3.append(model.predict_proba(cov_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(180, 32, 100)\n",
            "(60, 32, 100)\n",
            "(60, 32, 100)\n",
            "(180, 4656)\n",
            "(180,)\n",
            "(60, 4656)\n",
            "(60,)\n",
            "(60, 4656)\n",
            "(180, 96, 96)\n",
            "(60, 96, 96)\n",
            "(60, 96, 96)\n",
            "[14:51:16] WARNING: ../src/learner.cc:516: \n",
            "Parameters: { probability } might not be used.\n",
            "\n",
            "  This may not be accurate due to some parameters are only used in language bindings but\n",
            "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
            "  verification. Please open an issue if you find above cases.\n",
            "\n",
            "\n",
            "(180, 32, 100)\n",
            "(60, 32, 100)\n",
            "(60, 32, 100)\n",
            "(180, 4656)\n",
            "(180,)\n",
            "(60, 4656)\n",
            "(60,)\n",
            "(60, 4656)\n",
            "(180, 96, 96)\n",
            "(60, 96, 96)\n",
            "(60, 96, 96)\n",
            "[14:52:06] WARNING: ../src/learner.cc:516: \n",
            "Parameters: { probability } might not be used.\n",
            "\n",
            "  This may not be accurate due to some parameters are only used in language bindings but\n",
            "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
            "  verification. Please open an issue if you find above cases.\n",
            "\n",
            "\n",
            "(180, 32, 100)\n",
            "(60, 32, 100)\n",
            "(60, 32, 100)\n",
            "(180, 4656)\n",
            "(180,)\n",
            "(60, 4656)\n",
            "(60,)\n",
            "(60, 4656)\n",
            "(180, 96, 96)\n",
            "(60, 96, 96)\n",
            "(60, 96, 96)\n",
            "[14:53:04] WARNING: ../src/learner.cc:516: \n",
            "Parameters: { probability } might not be used.\n",
            "\n",
            "  This may not be accurate due to some parameters are only used in language bindings but\n",
            "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
            "  verification. Please open an issue if you find above cases.\n",
            "\n",
            "\n",
            "(180, 32, 100)\n",
            "(60, 32, 100)\n",
            "(60, 32, 100)\n",
            "(180, 4656)\n",
            "(180,)\n",
            "(60, 4656)\n",
            "(60,)\n",
            "(60, 4656)\n",
            "(180, 96, 96)\n",
            "(60, 96, 96)\n",
            "(60, 96, 96)\n",
            "[14:53:56] WARNING: ../src/learner.cc:516: \n",
            "Parameters: { probability } might not be used.\n",
            "\n",
            "  This may not be accurate due to some parameters are only used in language bindings but\n",
            "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
            "  verification. Please open an issue if you find above cases.\n",
            "\n",
            "\n",
            "(180, 32, 100)\n",
            "(60, 32, 100)\n",
            "(60, 32, 100)\n",
            "(180, 4656)\n",
            "(180,)\n",
            "(60, 4656)\n",
            "(60,)\n",
            "(60, 4656)\n",
            "(180, 96, 96)\n",
            "(60, 96, 96)\n",
            "(60, 96, 96)\n",
            "[14:54:48] WARNING: ../src/learner.cc:516: \n",
            "Parameters: { probability } might not be used.\n",
            "\n",
            "  This may not be accurate due to some parameters are only used in language bindings but\n",
            "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
            "  verification. Please open an issue if you find above cases.\n",
            "\n",
            "\n",
            "(180, 32, 100)\n",
            "(60, 32, 100)\n",
            "(60, 32, 100)\n",
            "(180, 4656)\n",
            "(180,)\n",
            "(60, 4656)\n",
            "(60,)\n",
            "(60, 4656)\n",
            "(180, 96, 96)\n",
            "(60, 96, 96)\n",
            "(60, 96, 96)\n",
            "[14:55:45] WARNING: ../src/learner.cc:516: \n",
            "Parameters: { probability } might not be used.\n",
            "\n",
            "  This may not be accurate due to some parameters are only used in language bindings but\n",
            "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
            "  verification. Please open an issue if you find above cases.\n",
            "\n",
            "\n",
            "(180, 32, 100)\n",
            "(60, 32, 100)\n",
            "(60, 32, 100)\n",
            "(180, 4656)\n",
            "(180,)\n",
            "(60, 4656)\n",
            "(60,)\n",
            "(60, 4656)\n",
            "(180, 96, 96)\n",
            "(60, 96, 96)\n",
            "(60, 96, 96)\n",
            "[14:56:37] WARNING: ../src/learner.cc:516: \n",
            "Parameters: { probability } might not be used.\n",
            "\n",
            "  This may not be accurate due to some parameters are only used in language bindings but\n",
            "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
            "  verification. Please open an issue if you find above cases.\n",
            "\n",
            "\n",
            "(180, 32, 100)\n",
            "(60, 32, 100)\n",
            "(60, 32, 100)\n",
            "(180, 4656)\n",
            "(180,)\n",
            "(60, 4656)\n",
            "(60,)\n",
            "(60, 4656)\n",
            "(180, 96, 96)\n",
            "(60, 96, 96)\n",
            "(60, 96, 96)\n",
            "[14:57:28] WARNING: ../src/learner.cc:516: \n",
            "Parameters: { probability } might not be used.\n",
            "\n",
            "  This may not be accurate due to some parameters are only used in language bindings but\n",
            "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
            "  verification. Please open an issue if you find above cases.\n",
            "\n",
            "\n",
            "(180, 32, 100)\n",
            "(60, 32, 100)\n",
            "(60, 32, 100)\n",
            "(180, 4656)\n",
            "(180,)\n",
            "(60, 4656)\n",
            "(60,)\n",
            "(60, 4656)\n",
            "(180, 96, 96)\n",
            "(60, 96, 96)\n",
            "(60, 96, 96)\n",
            "[14:58:21] WARNING: ../src/learner.cc:516: \n",
            "Parameters: { probability } might not be used.\n",
            "\n",
            "  This may not be accurate due to some parameters are only used in language bindings but\n",
            "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
            "  verification. Please open an issue if you find above cases.\n",
            "\n",
            "\n",
            "(180, 32, 100)\n",
            "(60, 32, 100)\n",
            "(60, 32, 100)\n",
            "(180, 4656)\n",
            "(180,)\n",
            "(60, 4656)\n",
            "(60,)\n",
            "(60, 4656)\n",
            "(180, 96, 96)\n",
            "(60, 96, 96)\n",
            "(60, 96, 96)\n",
            "[14:59:07] WARNING: ../src/learner.cc:516: \n",
            "Parameters: { probability } might not be used.\n",
            "\n",
            "  This may not be accurate due to some parameters are only used in language bindings but\n",
            "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
            "  verification. Please open an issue if you find above cases.\n",
            "\n",
            "\n",
            "(180, 32, 100)\n",
            "(60, 32, 100)\n",
            "(60, 32, 100)\n",
            "(180, 4656)\n",
            "(180,)\n",
            "(60, 4656)\n",
            "(60,)\n",
            "(60, 4656)\n",
            "(180, 96, 96)\n",
            "(60, 96, 96)\n",
            "(60, 96, 96)\n",
            "[15:00:03] WARNING: ../src/learner.cc:516: \n",
            "Parameters: { probability } might not be used.\n",
            "\n",
            "  This may not be accurate due to some parameters are only used in language bindings but\n",
            "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
            "  verification. Please open an issue if you find above cases.\n",
            "\n",
            "\n",
            "(180, 32, 100)\n",
            "(60, 32, 100)\n",
            "(60, 32, 100)\n",
            "(180, 4656)\n",
            "(180,)\n",
            "(60, 4656)\n",
            "(60,)\n",
            "(60, 4656)\n",
            "(180, 96, 96)\n",
            "(60, 96, 96)\n",
            "(60, 96, 96)\n",
            "[15:00:58] WARNING: ../src/learner.cc:516: \n",
            "Parameters: { probability } might not be used.\n",
            "\n",
            "  This may not be accurate due to some parameters are only used in language bindings but\n",
            "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
            "  verification. Please open an issue if you find above cases.\n",
            "\n",
            "\n",
            "(180, 32, 100)\n",
            "(60, 32, 100)\n",
            "(60, 32, 100)\n",
            "(180, 4656)\n",
            "(180,)\n",
            "(60, 4656)\n",
            "(60,)\n",
            "(60, 4656)\n",
            "(180, 96, 96)\n",
            "(60, 96, 96)\n",
            "(60, 96, 96)\n",
            "[15:01:47] WARNING: ../src/learner.cc:516: \n",
            "Parameters: { probability } might not be used.\n",
            "\n",
            "  This may not be accurate due to some parameters are only used in language bindings but\n",
            "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
            "  verification. Please open an issue if you find above cases.\n",
            "\n",
            "\n",
            "(180, 32, 100)\n",
            "(60, 32, 100)\n",
            "(60, 32, 100)\n",
            "(180, 4656)\n",
            "(180,)\n",
            "(60, 4656)\n",
            "(60,)\n",
            "(60, 4656)\n",
            "(180, 96, 96)\n",
            "(60, 96, 96)\n",
            "(60, 96, 96)\n",
            "[15:02:41] WARNING: ../src/learner.cc:516: \n",
            "Parameters: { probability } might not be used.\n",
            "\n",
            "  This may not be accurate due to some parameters are only used in language bindings but\n",
            "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
            "  verification. Please open an issue if you find above cases.\n",
            "\n",
            "\n",
            "(180, 32, 100)\n",
            "(60, 32, 100)\n",
            "(60, 32, 100)\n",
            "(180, 4656)\n",
            "(180,)\n",
            "(60, 4656)\n",
            "(60,)\n",
            "(60, 4656)\n",
            "(180, 96, 96)\n",
            "(60, 96, 96)\n",
            "(60, 96, 96)\n",
            "[15:03:33] WARNING: ../src/learner.cc:516: \n",
            "Parameters: { probability } might not be used.\n",
            "\n",
            "  This may not be accurate due to some parameters are only used in language bindings but\n",
            "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
            "  verification. Please open an issue if you find above cases.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0156095Y3-h",
        "outputId": "13180d19-91ce-4081-fe48-67fa3f27de3f"
      },
      "source": [
        "auc1=np.array(auc1)\n",
        "auc2=np.array(auc2)\n",
        "auc3=np.array(auc3)\n",
        "proba1=np.array(proba1)\n",
        "proba2=np.array(proba2)\n",
        "proba3=np.array(proba3)\n",
        "val_proba1=np.array(val_proba1)\n",
        "val_proba2=np.array(val_proba2)\n",
        "val_proba3=np.array(val_proba3)\n",
        "score1=np.array(score1)\n",
        "score2=np.array(score2)\n",
        "score3=np.array(score3)\n",
        "print(auc1.shape)\n",
        "print(auc2.shape)\n",
        "print(auc3.shape)\n",
        "print(proba1.shape)\n",
        "print(proba2.shape)\n",
        "print(proba3.shape)\n",
        "print(val_proba1.shape)\n",
        "print(val_proba2.shape)\n",
        "print(val_proba3.shape)\n",
        "print(score1.shape)\n",
        "print(score2.shape)\n",
        "print(score3.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15,)\n",
            "(15,)\n",
            "(15,)\n",
            "(15, 60, 2)\n",
            "(15, 60, 2)\n",
            "(15, 60, 2)\n",
            "(15, 60, 2)\n",
            "(15, 60, 2)\n",
            "(15, 60, 2)\n",
            "(15,)\n",
            "(15,)\n",
            "(15,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ow2nc5owJQ4L",
        "outputId": "081a73c8-c3f8-49a8-9af8-be8e7ea8db9e"
      },
      "source": [
        "score1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.41666667, 0.6       , 0.61666667, 0.51666667, 0.6       ,\n",
              "       0.76666667, 0.35      , 0.76666667, 0.66666667, 0.41666667,\n",
              "       0.45      , 0.48333333, 0.71666667, 0.45      , 0.48333333])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGpaK8ZYg71C",
        "outputId": "b45c5170-d82d-4a8b-87b4-b640e153a1b9"
      },
      "source": [
        "auc1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.48918269, 0.512     , 0.504     , 0.35403727, 0.61490683,\n",
              "       0.46118012, 0.41614907, 0.60714286, 0.50721154, 0.544     ,\n",
              "       0.558     , 0.49147727, 0.65625   , 0.37784091, 0.54403409])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3qwEGoJg62l",
        "outputId": "6b46aeef-1bcd-4daa-bc41-758d465eabbd"
      },
      "source": [
        "auc2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.53004808, 0.534     , 0.428     , 0.58307453, 0.62732919,\n",
              "       0.63354037, 0.44642857, 0.47670807, 0.41826923, 0.453     ,\n",
              "       0.704     , 0.4140625 , 0.68039773, 0.32528409, 0.39417614])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2RWYrTUcUkF",
        "outputId": "e7445a15-fb38-4a23-b10f-d202b93b1abb"
      },
      "source": [
        "auc3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.60456731, 0.439     , 0.531     , 0.47670807, 0.51552795,\n",
              "       0.57531056, 0.58618012, 0.51009317, 0.44230769, 0.409     ,\n",
              "       0.416     , 0.58806818, 0.65980114, 0.47727273, 0.52840909])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp_Sz4Djcoyw"
      },
      "source": [
        "proba4=[]\n",
        "proba5=[]\n",
        "val_proba4=[]\n",
        "val_proba5=[]\n",
        "auc=[]\n",
        "aucc=[]\n",
        "for i in range(15):\n",
        "  p1=proba1[i,:,1]\n",
        "  p2=proba2[i,:,1]\n",
        "  p3=proba3[i,:,1]\n",
        "  preds=(p1+p2+p3)/3\n",
        "  preds2=np.maximum(p1,p2,p3)\n",
        "  proba4.append(preds)\n",
        "  proba5.append(preds2)\n",
        "  p1=val_proba1[i,:,1]\n",
        "  p2=val_proba2[i,:,1]\n",
        "  p3=val_proba3[i,:,1]\n",
        "  preds=(p1+p2+p3)/3\n",
        "  preds2=np.maximum(p1,p2,p3)\n",
        "  val_proba4.append(preds)\n",
        "  val_proba5.append(preds2)\n",
        "  fpr, tpr, threshold = metrics.roc_curve(validation_labels, preds)\n",
        "  roc_auc = metrics.auc(fpr, tpr)\n",
        "  auc.append(roc_auc)\n",
        "  fpr, tpr, threshold = metrics.roc_curve(validation_labels, preds2)\n",
        "  roc_auc = metrics.auc(fpr, tpr)\n",
        "  aucc.append(roc_auc)\n",
        "auc=np.array(auc)\n",
        "aucc=np.array(aucc)\n",
        "proba4=np.array(proba4)\n",
        "proba5=np.array(proba5)\n",
        "val_proba4=np.array(val_proba4)\n",
        "val_proba5=np.array(val_proba5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnCv0jm5emH8",
        "outputId": "fb9c8faa-59cf-4f25-96f3-d66935e387d3"
      },
      "source": [
        "auc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.54829545, 0.31107955, 0.5       , 0.67755682, 0.31960227,\n",
              "       0.41619318, 0.421875  , 0.45170455, 0.52130682, 0.48295455,\n",
              "       0.45596591, 0.50852273, 0.6875    , 0.35085227, 0.45880682])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X06rXU2Gfy6D",
        "outputId": "41b557d3-e217-4e5f-c861-c550d708f60a"
      },
      "source": [
        "aucc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.53053977, 0.37073864, 0.49786932, 0.70596591, 0.35227273,\n",
              "       0.44602273, 0.69886364, 0.37784091, 0.50710227, 0.42471591,\n",
              "       0.36292614, 0.48650568, 0.70454545, 0.35653409, 0.47585227])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5YeqYoIBVHu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrdBgZzyAicX"
      },
      "source": [
        "sub=int(input(\"enter subject number:\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvnv5NZzEgOy"
      },
      "source": [
        "sub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4iXm4yNDqCl"
      },
      "source": [
        "auc1[sub-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpngcUzCDshQ"
      },
      "source": [
        "auc2[sub-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLSQiBkaDvaP"
      },
      "source": [
        "auc3[sub-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4to5Mj7IDwgC"
      },
      "source": [
        "auc[sub-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVLspOfvDy8q"
      },
      "source": [
        "aucc[sub-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlKVCfULD1-N"
      },
      "source": [
        "proba3[sub-1,:,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3IiYLoAE75G"
      },
      "source": [
        "for i in range(15):\n",
        "  print(max(auc1[i],auc2[i],auc3[i],auc[i],aucc[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvoAtVpUNNqL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "670cee6a-17fb-4bea-f4d3-ab40fb57fd2c"
      },
      "source": [
        "score1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.41666667, 0.6       , 0.61666667, 0.51666667, 0.6       ,\n",
              "       0.76666667, 0.35      , 0.76666667, 0.66666667, 0.41666667,\n",
              "       0.45      , 0.48333333, 0.71666667, 0.45      , 0.48333333])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    }
  ]
}